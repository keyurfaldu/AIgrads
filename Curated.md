* [Specializing Word Embeddings (for Parsing) by Information Bottleneck](https://www.aclweb.org/anthology/D19-1276.pdf), Best paper EMNLP 2019

* [Designing and Interpreting Probes with Control Tasks](https://www.aclweb.org/anthology/D19-1275.pdf) : Runner up paper EMNLP 2019

* [AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models](https://www.aclweb.org/anthology/D19-3002.pdf) 

* [Investigating BERTâ€™s Knowledge of Language: Five Analysis Methods with NPIs](https://arxiv.org/pdf/1909.02597.pdf)

* [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909): 3000 citations

* [MT-DNNKD: Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding](https://arxiv.org/pdf/1904.09482.pdf)

* [BAN: Born Again Neural Networks](https://arxiv.org/abs/1805.04770) : one of the initial distillation paper

* [Unifying Question Answering, Text Classification, and Regression via Span Extraction](https://arxiv.org/pdf/1904.09286.pdf)

* [Multilingual Neural Machine Translation with Knowledge Distillation](https://arxiv.org/abs/1902.10461)

* Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks

* What does BERT learn about the structure of language? [Aug-2019,]

* Analyzing the Structure of Attention in a Transformer Language Model

* Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned

* Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?, ACL 2020

* Towards Transparent and Explainable Attention Models, ACL 2020

* Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words, ACL 2020

* Roles and Utilization of Attention Heads in Transformer-based Neural Language Models, ACL 2020

* Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT, ACL 2020

* [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461), 2020


