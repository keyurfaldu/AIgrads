**DEEP LEARNING BUILDING BLOCKS**
* [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980), 2014, [49687 citations]

* [Additive Attention: Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473), ICLR 2015, [13149 citations]

* [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](https://arxiv.org/abs/1312.6120) ICLR2014 [894 citations]

* [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909), ACL 2016, 3000 citations **BPE Paper**

* [Layer Normalization](https://arxiv.org/abs/1607.06450), Hinton, 2016, [1959 citations]

* [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022), 2016, [900 citations]

* [Nesterov's Accelerated Gradient and Momentum as approximations to Regularised Update Descent](https://arxiv.org/abs/1607.01981), 2016, [22 citations]

* [Wordpiece: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144), Google, 2016, (2974 citations)

* [All you need is a good init](https://arxiv.org/abs/1511.06422), ICLR 2016 [386 citations]

* [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412), 2017, [881 citations]

* [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515), NIPS 2017, [991 Citations]

* [L2 Regularization versus Batch and Weight Normalization](https://arxiv.org/abs/1706.05350), 2017, [80 citations]

* [Group Normalization](https://arxiv.org/abs/1803.08494), Kaiming He, 2018 [674 citations]

* [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612), 2018, [210 citations]

* [Norm matters: efficient and accurate normalization schemes in deep networks](https://arxiv.org/abs/1803.01814), NIPS 2018, [65 citations]

* [Three Mechanisms of Weight Decay Regularization](https://arxiv.org/abs/1810.12281), 2018, [45 citations]

* [Fixup Initialization: Residual Learning Without Normalization](https://arxiv.org/abs/1901.09321), [ICLR 2019] [87 Citations]

**NN Architecture**
* [PAY LESS ATTENTION WITH LIGHTWEIGHT AND DYNAMIC CONVOLUTIONS](https://arxiv.org/pdf/1901.10430.pdf), Facebook AI Research

* [Rethinking complex neural network architectures for document classification](https://www.aclweb.org/anthology/N19-1408/), NAACL 2019


**PRE-TRAINING**

* [BAN: Born Again Neural Networks](https://arxiv.org/abs/1805.04770) : one of the initial distillation paper

* [Multilingual Neural Machine Translation with Knowledge Distillation](https://arxiv.org/abs/1902.10461)

* [Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848), 2019

* [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291), 2019

* [Semi-Supervised Sequence Modeling with Cross-View Training](https://arxiv.org/abs/1809.08370), 2018, EMNLP 2018

* [Semi-supervised Multitask Learning for Sequence Labeling](https://arxiv.org/abs/1704.07156)

* [Unsupervised Pretraining for Sequence to Sequence Learning](https://www.aclweb.org/anthology/D17-1039/), EMNLP 2017

* [DeFINE: Deep Factorized Input Token Embeddings for Neural Sequence Modeling](https://openreview.net/forum?id=rJeXS04FPH), ICLR 2020

* [Depth-Adaptive Transformer](https://arxiv.org/abs/1910.10073), ICLR 2020

* [FreeLB: Enhanced Adversarial Training for Natural Language Understanding](https://arxiv.org/abs/1909.11764), ICLR 2020

* [A Mutual Information Maximization Perspective of Language Representation Learning](https://arxiv.org/abs/1910.08350), ICLR 2020

* [Mogrifier LSTM](https://arxiv.org/abs/1909.01792), ICLR 2020

* [Enhanced LSTM for Natural Language Inference](https://arxiv.org/abs/1609.06038), ACL 2017, 450 citations

**FINETUNING / TRANSFER-LEARNING**

* [BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671), ICML 2019, (50 citations)

* [BERT Post-Training for Review Reading Comprehension and Aspect-based Sentiment Analysis](https://arxiv.org/abs/1904.02232), NAACL 2019 (63 citations)

* [Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling](https://arxiv.org/abs/1812.10860) ACL 2019 (18 citations)

* [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/pdf/1811.01088.pdf), 2018

* [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461), 2020

* [Learning and Evaluating General Linguistic Intelligence](https://arxiv.org/pdf/1901.11373.pdf), 2019

* [Learning from Dialogue after Deployment: Feed Yourself, Chatbot!](https://arxiv.org/abs/1901.05415), ACL 2019

* [Syntactic Scaffolds for Semantic Structures](https://arxiv.org/abs/1808.10485), EMNLP 2018

* [HellaSwag: Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830), ACL 2019

* [Linguistic Knowledge and Transferability of Contextual Representations](https://arxiv.org/abs/1903.08855), NAACL 2019

* [An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models](https://arxiv.org/abs/1902.10547), NAACL 2019

* [Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks](https://arxiv.org/abs/1811.01088v2), 2019

* [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://arxiv.org/abs/1903.05987), 2019

* [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751), 2019

**MISC**

* [Semi-supervised sequence tagging with bidirectional language models](https://arxiv.org/abs/1705.00108), ACL 2017

* [A PROBABILISTIC FORMULATION OF UNSUPERVISED TEXT STYLE TRANSFER](https://arxiv.org/pdf/2002.03912.pdf), ICLR 2020


**MULTI-TASK-LEARNNG**

* [BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning](https://arxiv.org/abs/1902.02671), ICML 2019

* [Latent Multi-task Architecture Learning](https://arxiv.org/abs/1705.08142), AAAI 2019

**Representation**


* [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf), 2018 [485 citations]

* [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf), 2019, [155 citations]

* [pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference](https://arxiv.org/abs/1810.08854), 2019, NAACL

* [ENCODING WORD ORDER IN COMPLEX EMBEDDINGS](https://arxiv.org/pdf/1912.12333.pdf), ICLR 2020


**NLP Downstream tasks** 

* [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635), ICLR 2019

* [Ultra-Fine Entity Typing](https://arxiv.org/abs/1807.04905), ACL 2018 [48 citations]

* [Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing](https://arxiv.org/abs/1903.02591), NAACL 2019

* [Dynamic Meta-Embeddings for Improved Sentence Representations](https://arxiv.org/abs/1804.07983), EMNLP 2018

* [THE CURIOUS CASE OF NEURAL TEXT DeGENERATION](https://arxiv.org/pdf/1904.09751.pdf), ICLR 2020


**INTERPRETABILITY**

* [Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?](https://arxiv.org/abs/2004.03685), ACL 2020, Yoav Goldberg

* [TOWARDS BETTER UNDERSTANDING OF GRADIENT-BASED ATTRIBUTION METHODS FOR DEEP NEURAL NETWORKS](https://openreview.net/pdf?id=Sy21R9JAW), ICLR 2018

* [Is Attention Interpretable?](https://arxiv.org/abs/1906.03731), ACL 2019, [62 citations]

* [Specializing Word Embeddings (for Parsing) by Information Bottleneck](https://www.aclweb.org/anthology/D19-1276.pdf), **Best paper EMNLP 2019**

* [What does BERT learn about the structure of language?](https://hal.inria.fr/hal-02131630/document) [Aug-2019] [87 Citations]

* [Analyzing the Structure of Attention in a Transformer Language Model](https://arxiv.org/abs/1906.04284) [ACL-2019] [25 Citations]

* [Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned](https://arxiv.org/pdf/1905.09418.pdf) [ACL-2019] [96 Citations]

* [Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words](https://arxiv.org/abs/2005.01810), ACL 2020

* [Roles and Utilization of Attention Heads in Transformer-based Neural Language Models](https://www.aclweb.org/anthology/2020.acl-main.311/), ACL 2020

* [Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT](https://arxiv.org/abs/2004.14786), ACL 2020

* [WHAT CAN NEURAL NETWORKS REASON ABOUT?](https://arxiv.org/pdf/1905.13211.pdf), ICLR 2020

* [TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP](https://arxiv.org/pdf/2005.05909.pdf)

* [Attention in Natural Language Processing](https://arxiv.org/pdf/1902.02181.pdf) Referred by Prof Amit Sheth

**e-Learning**
* Mastering Rate based Curriculum Learning


**Question Generation**
* Recent Advances in Neural Question Generation
* Semantic Graphs for Generating Deep Questions
* HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering
* Toward Subgraph Guided Knowledge Graph Question Generation with Graph Neural Networks
* Question Generation for Question Answering
* Machine Comprehension by Text-to-Text Neural Question Generation
* Improving Neural Question Generation Using Answer Separation
* CopyBERT: A Unified Approach to Question Generation with Self-Attention
* Learning to Automatically Generate Fill-In-The-Blank Quizzes
* Difficulty-aware Distractor Generation for Gap-Fill Items
* A Systematic Review of Automatic Question Generation for Educational Purposes
* Knowledge-Driven Distractor Generation for Cloze-style Multiple Choice Questions

**Question Answering**
* Simple and Effective Multi-Paragraph Reading Comprehension 

**KG + NLP**

* SCITAIL: A Textual Entailment Dataset from Science Question Answering

* Improving Question Answering by Commonsense-Based Pre-Training

* Graph-Based Reasoning over Heterogeneous External Knowledge for Commonsense Question Answering

* EXPLOITING STRUCTURED KNOWLEDGE IN TEXT VIA GRAPH-GUIDED REPRESENTATION LEARNING

* WorldTree V2: A Corpus of Science-Domain Structured Explanations and Inference Patterns supporting Multi-Hop Inference

* SemEval-2020 Task 4: Commonsense Validation and Explanation

* ECNU-SenseMaker at SemEval-2020 Task 4: Leveraging Heterogeneous Knowledge Resources for Commonsense Validation and Explanation

**Best NLP Papers ACL 2019 & 2020**

* [How Can We Accelerate Progress Towards Human-like Linguistic Generalization?](https://arxiv.org/pdf/2005.00955.pdf)

**Suggestions from Prof Amit Sheth**
* [Attention in Natural Language Processing](https://arxiv.org/pdf/1902.02181.pdf) 2020




