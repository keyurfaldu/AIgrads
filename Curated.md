* [Specializing Word Embeddings (for Parsing) by Information Bottleneck](https://www.aclweb.org/anthology/D19-1276.pdf), Best paper EMNLP 2019

* [Designing and Interpreting Probes with Control Tasks](https://www.aclweb.org/anthology/D19-1275.pdf) : Runner up paper EMNLP 2019

* [AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models](https://www.aclweb.org/anthology/D19-3002.pdf) 

* [Investigating BERTâ€™s Knowledge of Language: Five Analysis Methods with NPIs](https://arxiv.org/pdf/1909.02597.pdf)

* [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909): 3000 citations

* [MT-DNNKD: Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding](https://arxiv.org/pdf/1904.09482.pdf)

* [BAN: Born Again Neural Networks](https://arxiv.org/abs/1805.04770) : one of the initial distillation paper

* [Unifying Question Answering, Text Classification, and Regression via Span Extraction](https://arxiv.org/pdf/1904.09286.pdf)

* [Multilingual Neural Machine Translation with Knowledge Distillation](https://arxiv.org/abs/1902.10461)

* Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks