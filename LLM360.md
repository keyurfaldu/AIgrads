## LLM360: Towards Fully Transparent Open-Source LLMs
### Liu, Zhengzhong, et al. 
### arXiv preprint arXiv:2312.06550 (2023) [[PDF](https://arxiv.org/pdf/2312.06550.pdf)].


**Whats Unique**
* Making end-to-end LLM training process transparent and reproducible
* Enable researchers to get access to training data, data mix, hyper parameters, check points, code, configuration, training metrics and insights into errors like NaN while trainig data chunks
* Ability to leverage middle checkpoints to train domain specific LLMs
* Released LLM360 framework along with two models
    * AMBER: 7B english model
    * CRYSTALCODER: 7B english and code model
* Following templates gives a big picture on what all is covered under LLM360
   <p align="center">
    <img width=600 src="images/LLM360_table_1.png">
    <em>Source: Author</em>
    </p>
* Internal checkpoints also enable studying behavior like memorization behavior of the model.
* The leaderboard performance of LLM360 could be seen as below:
   <p align="center">
    <img width=600 src="images/LLM360_leaderboard.png">
    <em>Source: Author</em>
    </p>

