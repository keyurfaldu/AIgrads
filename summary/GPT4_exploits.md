## Exploiting Novel GPT-4 APIs.
### Pelrine, Kellin, et al
### arXiv preprint arXiv:2312.14302 (2023) [[PDF](https://arxiv.org/pdf/2312.14302.pdf)].


Key Points

* It is very easy to exploit the APIs on LLMs like GPT4. That exposes the vulnerability of the LLM model and demands more attention to make it secure. 
* Three APIs were studied 
    * Fine Tuning
    * Function Calling
    * Knowledge Retrieval
* Fine tuning: It was very easy to turn GPT4 to give harmful (how to make a bomb), biased outputs (negative narratives on Hillary Clinton) and also divulge private (eg. revealing email ids) information by fine tuninig it with miniscule data. Its also easy to neutralize the guardrails on the output behavior of LLMs like be responsible, truthful etc. 
* Function Calling: It was very easy to modify function call instruction, i.e. manipulative URLs can be injucted in code instructions, or transfer money.
* Knowledge Retrieval: Even if the LLM is asked to summarise file by retrieving it, the content in the file can further override the instructions given to GPT, it can give incorrect summary or can further ask LLM to do undesired function calling. 
* Following figure captures these vulnerabilities

   <p align="center">
    <img width=600 src="images/GPT4_explots.png">
    <em>Source: Author</em>
    </p>

